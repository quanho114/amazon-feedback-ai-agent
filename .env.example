# ===========================================
# Amazon AI System Agent - Configuration
# ===========================================

# --- Main LLM (Heavy Tasks) ---
# Used for: Sentiment Analysis, RAG, Insights, Summarization, Analytics
MEGALLM_API_KEY=your_main_api_key_here
MEGALLM_BASE_URL=https://ai.megallm.io/v1
MEGALLM_MODEL=gemini-pro

# --- Fast Chat LLM (Optional) ---
# Used for: Quick conversations, greetings, general chat
# If not set, will fallback to main LLM config
# Recommended: Use faster/cheaper model here
CHAT_API_KEY=your_chat_api_key_here
CHAT_BASE_URL=https://ai.megallm.io/v1
CHAT_MODEL=gemini-1.5-flash

# ===========================================
# Configuration Strategies
# ===========================================

# Strategy 1: Single API Key (Simplest)
# - Use same key for both MEGALLM_API_KEY and CHAT_API_KEY
# - Just change model: gemini-pro vs gemini-1.5-flash

# Strategy 2: Separate Keys (Best Performance)
# - Main: Powerful model (gemini-pro, gpt-4)
# - Chat: Fast model (gemini-flash, gpt-3.5-turbo)
# - Benefits: Faster chat + lower cost

# Strategy 3: Different Providers
# - Main: Gemini Pro (better analysis)
# - Chat: OpenAI GPT-3.5 (faster API)
# - Mix and match based on task

# ===========================================
# Example Configurations
# ===========================================

# Example 1: All Gemini (Recommended for beginners)
# MEGALLM_API_KEY=sk-xxx
# MEGALLM_BASE_URL=https://ai.megallm.io/v1
# MEGALLM_MODEL=gemini-pro
# CHAT_API_KEY=sk-xxx  # Same key
# CHAT_BASE_URL=https://ai.megallm.io/v1
# CHAT_MODEL=gemini-1.5-flash

# Example 2: Mixed providers (Advanced)
# MEGALLM_API_KEY=gemini-key-xxx
# MEGALLM_BASE_URL=https://generativelanguage.googleapis.com/v1
# MEGALLM_MODEL=gemini-pro
# CHAT_API_KEY=sk-openai-xxx
# CHAT_BASE_URL=https://api.openai.com/v1
# CHAT_MODEL=gpt-3.5-turbo

# ===========================================
# Performance Tips
# ===========================================

# 1. For fastest chat responses:
#    CHAT_MODEL=gemini-1.5-flash (0.5-1s response)

# 2. For best analysis quality:
#    MEGALLM_MODEL=gemini-pro or gpt-4

# 3. For cost optimization:
#    - Chat: gemini-flash or gpt-3.5-turbo
#    - Analysis: gemini-pro

# 4. For rate limit management:
#    Use different API keys to separate quotas
