# File: src/test_model.py
from langchain_openai import ChatOpenAI
from src.config import MEGALLM_API_KEY, MEGALLM_BASE_URL, MEGALLM_MODEL

def test_megallm():
    print(f"üöÄ ƒêang g·ª≠i tin nh·∫Øn t·ªõi model: {MEGALLM_MODEL}...")
    
    # 1. Kh·ªüi t·∫°o k·∫øt n·ªëi
    try:
        llm = ChatOpenAI(
            api_key=MEGALLM_API_KEY,
            base_url=MEGALLM_BASE_URL,
            model=MEGALLM_MODEL,
            temperature=0.7 # ƒê·ªô s√°ng t·∫°o
        )

        # 2. G·ª≠i th·ª≠ m·ªôt c√¢u h·ªèi
        question = "Xin ch√†o, h√£y gi·ªõi thi·ªáu ng·∫Øn g·ªçn b·∫°n l√† ai?"
        print(f"üë§ User: {question}")
        
        response = llm.invoke(question)
        
        # 3. In c√¢u tr·∫£ l·ªùi
        print("\n" + "="*30)
        print(f"ü§ñ MegaLLM tr·∫£ l·ªùi:\n{response.content}")
        print("="*30 + "\n")
        print("‚úÖ CH√öC M·ª™NG! Model ho·∫°t ƒë·ªông ngon l√†nh.")

    except Exception as e:
        print("\n‚ùå L·ªñI K·∫æT N·ªêI R·ªíI!")
        print(f"Chi ti·∫øt l·ªói: {e}")
        print("üëâ G·ª£i √Ω: Ki·ªÉm tra l·∫°i API Key ho·∫∑c Base URL xem c√≥ ƒë√∫ng c·ªßa MegaLLM kh√¥ng.")

if __name__ == "__main__":
    test_megallm()